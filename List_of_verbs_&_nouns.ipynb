{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "List of verbs & nouns",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKFHVfJWJC8BMbgms+tqfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lupis30puc/ultimate_recipe/blob/master/List_of_verbs_%26_nouns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp-lLNAPZr5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install textacy"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JtEc7UEAVSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json #for loading\n",
        "import pandas as pd\n",
        "import re #for preprocessing\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, strip_short\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import en_core_web_sm\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "import spacy \n",
        "from spacy.attrs import ORTH\n",
        "from collections import Counter\n",
        "import textacy\n",
        "import numpy as np \n",
        "import csv"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwsCn7vfAbtL",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7eb6f526-6408-4b1a-8fcf-bbc7d9643ea9"
      },
      "source": [
        "uploaded = files.upload() "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87a18051-35b5-4477-b2f9-e802cecda5ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-87a18051-35b5-4477-b2f9-e802cecda5ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Lemma_instructions_scentence.pkl to Lemma_instructions_scentence.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoJjNnTxAdUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "576985a7-0cd6-46fe-de19-f1bcd78877f6"
      },
      "source": [
        "# Load the local file\n",
        "sent_lem = pickle.load(open(\"Lemma_instructions_scentence.pkl\", \"rb\"))\n",
        "print(sent_lem[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "place chicken butter soup onion slow cooker fill enough water cover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRogSG1uAfBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7e01cc0-affb-4585-9a50-f746f84d97ec"
      },
      "source": [
        "len(sent_lem)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "341454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpVbyoUjA1Lz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9297ba34-661d-4ffe-e583-c878b4a3a061"
      },
      "source": [
        "# trying out spacy package for speech tagging \n",
        "  \n",
        "# Load English tokenizer, tagger,  \n",
        "# parser, NER and word vectors \n",
        "nlp = spacy.load(\"en_core_web_sm\")   â€©\n",
        "\n",
        "sentence = sent_lem[0]\n",
        "\n",
        "doc_s = nlp(sentence)\n",
        "verbs_2 = []\n",
        "nouns_2 = []\n",
        "\n",
        "for token in doc_s:\n",
        "  if token.pos_ == \"VERB\":\n",
        "    verbs_2.append(token)\n",
        "  elif token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "    nouns_2.append(token)\n",
        "    #print(token, token.pos_) \n",
        "\n",
        "print(verbs_2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fill]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQe-FmvkKY9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8693f6d-fbe4-41f6-fffb-933253ec8323"
      },
      "source": [
        "print(nouns_2)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[place, chicken, butter, soup, onion, cooker, water, cover]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in5qYnHeVsmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f7cffb3-728b-4866-d2f9-3e15f7db4f50"
      },
      "source": [
        "# try leaving unique words in the example sentence:\n",
        "\n",
        "words_ex = [token.text for token in doc_s if not token.is_stop and not token.is_punct]\n",
        "word_freq_ex = Counter(words_ex)\n",
        "unique_words_ex = [word for (word, freq) in word_freq_ex.items() if freq == 1]\n",
        "print(sorted(unique_words_ex))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['butter', 'chicken', 'cooker', 'cover', 'fill', 'onion', 'place', 'slow', 'soup', 'water']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ooF_QPWoRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0c76549-e6f8-491f-ad3e-77a8cd2b5cc1"
      },
      "source": [
        "print(len(words_ex))\n",
        "print(len(unique_words_ex))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoKrRrc9nrjL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "555c9b3f-7cb5-403d-836f-ef65ea6c43c0"
      },
      "source": [
        "#v_c = uw_doc.count_by(ORTH)\n",
        "c_ex = Counter()\n",
        "for token in doc:\n",
        "    c_ex[token.orth_] += 1 # Equivalently, token.text\n",
        "\n",
        "print(sorted(c_ex))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['biscuit', 'butter', 'center', 'chicken', 'cook', 'cooker', 'cover', 'dough', 'enough', 'fill', 'high', 'hour', 'longer', 'minute', 'onion', 'place', 'raw', 'serve', 'slow', 'soup', 'tear', 'water']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZEt7g37n4kD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbfac60c-14a2-44f4-e7ab-75ff3c59b129"
      },
      "source": [
        "print(len(unique_words_ex))\n",
        "print(len(c_ex))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb7vsm_IFpyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c9586bd1-6183-451d-c6d3-6e7bd4f83b4f"
      },
      "source": [
        "# try to iterate in all the data set!\n",
        "verbs = []\n",
        "nouns = []\n",
        "tokens = []\n",
        "\n",
        "# Process whole documents  \n",
        "for s in sent_lem:\n",
        "  doc = nlp(s) \n",
        "  for token in doc: \n",
        "    if token.pos_ == \"VERB\":\n",
        "      #if token.pos_ not in verbs:\n",
        "      verbs.append(token)\n",
        "    elif token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "      nouns.append(token)\n",
        "    else:\n",
        "      tokens.append(token)\n",
        "    #print(token, token.pos_) \n",
        "  \n",
        "# You want list of Verb tokens \n",
        "#print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])\n",
        "print(len(verbs))\n",
        "print(len(nouns))\n",
        "print(len(tokens))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "311515\n",
            "1752001\n",
            "370501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sy4kXaAGB9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('verb_list', 'w', newline='') as verbs1:\n",
        "     wr = csv.writer(verbs1, quoting=csv.QUOTE_ALL)\n",
        "     wr.writerow(verbs)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5z_K2wXUJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('noun_list', 'w', newline='') as nouns1:\n",
        "     wr = csv.writer(nouns1, quoting=csv.QUOTE_ALL)\n",
        "     wr.writerow(nouns)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RlUF6-rYGbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBv_SROgYDwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nouns_sort = sorted(nouns)\n",
        "nouns_final = list(dict.fromkeys(nouns_sort))\n",
        "print(nouns_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSupyEPaWw62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "2da9496e-123e-4bf3-f359-f793abb1dd53"
      },
      "source": [
        "# try getting verb chunks from example sentence:\n",
        "\n",
        "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
        "#sentence_doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')\n",
        "#verb_phrases = textacy.extract.pos_regex_matches(sentence_doc, pattern)\n",
        "verb_phrases = textacy.extract.pos_regex_matches(doc, pattern)\n",
        "\n",
        "# Print all Verb Phrase\n",
        "for chunk in verb_phrases:\n",
        "  print(chunk.text)\n",
        "\n",
        "print('------')\n",
        "# Extract Noun Phrase to explain what nouns are involved\n",
        "for chunk in doc.noun_chunks:\n",
        "  print (chunk)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fill\n",
            "serve\n",
            "slow\n",
            "cook\n",
            "------\n",
            "place chicken butter soup onion slow cooker\n",
            "enough water cover cover cook hour\n",
            "high minute\n",
            "place\n",
            "tear biscuit dough\n",
            "cooker cook dough\n",
            "longer raw center\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/textacy/extract.py:332: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
            "  action=\"once\",\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ZHi-mCbqdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "085a9372-3319-4ef6-921c-d0da9fae08b7"
      },
      "source": [
        "extract_d = data_lemma[:2]\n",
        "print(extract_d[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "slow cooker mix cream mushroom soup dry onion soup mix water place pot roast slow cooker coat soup mixture cook high set hour low setting hour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkaDuR6zZIYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try leaving unique words in the first 2 sentences:\n",
        "uni_words = []\n",
        "\n",
        "for s in extract_d:\n",
        "  doc = nlp(s)\n",
        "  words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "  word_freq = Counter(words)\n",
        "  unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "  for word in unique_words:\n",
        "    uni_words.append(word)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipizf0CecUf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1VEqJcijM25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0c2aa254-d7c3-442e-aa23-bf3185c3591c"
      },
      "source": [
        "verb_list = []\n",
        "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
        "\n",
        "uw_doc = textacy.make_spacy_doc(str(uni_words), lang='en_core_web_sm')\n",
        "verb_phrases = textacy.extract.pos_regex_matches(uw_doc, pattern)\n",
        "for chunk in verb_phrases:\n",
        "  verb_list.append(chunk.text)\n",
        "  #print(chunk.text)\n",
        "print(len(uw_doc))\n",
        "print(len(verb_list))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/textacy/extract.py:332: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
            "  action=\"once\",\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R43ZGcjlvLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c7ab453b-8856-49f1-9848-f7b89977b8bb"
      },
      "source": [
        "#v_c = uw_doc.count_by(ORTH)\n",
        "\n",
        "v_c = Counter()\n",
        "for token in uw_doc:\n",
        "    v_c[token.orth_] += 1 # Equivalently, token.text\n",
        "\n",
        "print(v_c)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({\"'\": 60, ',': 29, 'onion': 2, 'water': 2, 'high': 2, '[': 1, 'chicken': 1, 'butter': 1, 'soup': 1, 'fill': 1, 'hour': 1, 'minute': 1, 'serve': 1, 'tear': 1, 'biscuit': 1, 'longer': 1, 'raw': 1, 'center': 1, 'cream': 1, 'mushroom': 1, 'dry': 1, 'place': 1, 'pot': 1, 'roast': 1, 'coat': 1, 'mixture': 1, 'cook': 1, 'set': 1, 'low': 1, 'setting': 1, ']': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vCHmWzZfV7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b263a62-30cd-45b0-eef6-036dfbd8e70b"
      },
      "source": [
        "uw_verbs = []\n",
        "uw_nouns = []\n",
        "\n",
        "for token in uw_doc: \n",
        "    if token.pos_ == \"VERB\":\n",
        "      #if token.pos_ not in verbs:\n",
        "      uw_verbs.append(token)\n",
        "    elif token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "      uw_nouns.append(token)\n",
        "\n",
        "print(len(uw_verbs))\n",
        "print(len(uw_nouns))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xPponthkbJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4bb15ae1-55d2-4fe4-8f3f-d3fae02a8ddc"
      },
      "source": [
        "print(uw_verbs)\n",
        "print(sorted(uw_nouns))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[serve, setting]\n",
            "[chicken, butter, soup, onion, fill, water, hour, minute, tear, biscuit, center, cream, mushroom, onion, water, place, pot, roast, coat, mixture, cook, set]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8hG6De0e7hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5844e8a7-767f-4d86-8594-9fe408be8170"
      },
      "source": [
        "# try making the verb list without filtering unique words first, for the 2 sentences:\n",
        "w_list = []\n",
        "for s in extract_d:\n",
        "  doc = nlp(s)\n",
        "  words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "  for word in words:\n",
        "    w_list.append(word)\n",
        "verb_list_3 = []\n",
        "w_doc = textacy.make_spacy_doc(str(w_list), lang='en_core_web_sm')\n",
        "verb_phrases_3 = textacy.extract.pos_regex_matches(w_doc, pattern)\n",
        "\n",
        "#verb_list.append(chunk.text for chunk in verb_phrases)\n",
        "  #if token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "   #   nouns.append(token)\n",
        "for chunk in verb_phrases_3:\n",
        "  verb_list_3.append(chunk.text)\n",
        "#  print(chunk.text)\n",
        "#print(len(list(uni_words)))\n",
        "print(verb_list_3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['serve', 'setting']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/textacy/extract.py:332: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
            "  action=\"once\",\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "admep6ibeHdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c5349f6-ada6-4df4-e275-3adbbce7892c"
      },
      "source": [
        "print(len(w_list))\n",
        "print(len(uni_words))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft9Nat_Mc4TB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f405f6e2-f3a2-4881-f4d5-783f4a16dd80"
      },
      "source": [
        "# Try iteration checking for these 2 sentences:\n",
        "verbs_it = []\n",
        "nouns_it = []\n",
        "tokens_it = []\n",
        "\n",
        "# Process whole documents  \n",
        "for s in extract_d:\n",
        "  doc = nlp(s) \n",
        "  for token in doc: \n",
        "    if token.pos_ == \"VERB\":\n",
        "      #if token.pos_ not in verbs:\n",
        "      verbs_it.append(token)\n",
        "    elif token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "      nouns_it.append(token)\n",
        "    else:\n",
        "      tokens_it.append(token)\n",
        "    #print(token, token.pos_) \n",
        "  \n",
        "# You want list of Verb tokens \n",
        "#print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])\n",
        "print(len(verbs_it))\n",
        "print(verbs_it)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "[fill, serve, slow, cook, cook]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vggZCVL_c1N2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53c4b494-fb70-47c6-ca85-549c4bb0c905"
      },
      "source": [
        "# After obtaining the list of verbs, eliminating duplicatess:\n",
        "v_freq_it = Counter(verbs_it)\n",
        "uni_verbs_it = [word for (word, freq) in v_freq_it.items() if freq == 1]\n",
        "print(uni_verbs_it)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fill, serve, slow, cook, cook]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmq4W8gvf7ET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9887a803-4b9f-466a-ced2-1164c38baa16"
      },
      "source": [
        "print(v_freq_it)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({fill: 1, serve: 1, slow: 1, cook: 1, cook: 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_fmYAZiZoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3337ece1-a0d8-4a8d-965d-ebbe55d03ac9"
      },
      "source": [
        "# function to get unique values \n",
        "def unique(list1): \n",
        "    x = np.array(list1) \n",
        "    print(np.unique(x)) \n",
        "\n",
        "v_uni_n = unique(verbs_it)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fill serve cook slow cook]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18jWOYvFNAIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python code to remove duplicate elements \n",
        "def Remove(duplicate): \n",
        "    final_list = [] \n",
        "    for num in duplicate: \n",
        "        if num not in final_list: \n",
        "            final_list.append(num) \n",
        "    return final_list\n",
        "\n",
        "tokens = []\n",
        "\n",
        "# Process whole documents  \n",
        "for s in data_lemma:\n",
        "  doc = nlp(s) \n",
        "  for token in doc: \n",
        "    tokens.append(token)\n",
        "\n",
        "tokens_uni = Remove(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}